[
  {
    "path": "posts/welcome/",
    "title": "A/B Testing",
    "description": "A brief look into A/B testing and implementation of some statistical tests in R",
    "author": [
      {
        "name": "Campbell Miller",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-05-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nData Analysis and\r\nVisualization\r\nStatistical\r\nTests\r\nConclusions\r\n\r\nIntroduction\r\n\r\n\r\nShow code\r\n\r\n#loading data/packages\r\npacman::p_load(here, ggplot2, dplyr, broom)\r\n\r\ncat_data <- read.csv(here(file = \"cookie_cats.csv\"))\r\n\r\n\r\n\r\nA/B testing is a randomized experiment in which two or more versions\r\nof the object of interest are assigned to randomized segments of the\r\npopulation of interest. of the object of interest in the experiment. The\r\ntypical A/B testing experiment consists of a website in which there are\r\ndifferent versions of a website layout and the experimenter notes which\r\nversion leads to a higher percentage of the outcome of interest. In\r\npractice this could be seen by adding something like a cute cat picture\r\nto the button that asks for donations and randomly selecting who gets\r\nthe cute cat and who gets the plain donate button and comparing the\r\namount of donations earned in each case. If you would like to see all of\r\nthe code used or download the data to test for yourself feel free to\r\ncheck out my Github.\r\nThe data used in this example is from a Datacamp example\r\nlooking at A/B testing in python. Here I will be replicating some of the\r\nmethods in R and hope to reach the same conclusion. The data is from a\r\nmobile game called Cookie Cats. The experiment that is put into place\r\ninvolves the placement of a gate in the game that makes a player wait\r\nsome time before they continue to advance once reaching the gate. The\r\ninitial placement of the gate was at level 30, and the A/B test involves\r\nmoving the gate to level 40. The outcome of interest is player retention\r\nand there is data that shows if the player opened the app both one day\r\nand seven days after initially downloading the game. By randomly\r\nassigning who gets the gate at level 30 and who gets it at level 40, we\r\ncan hopefully see if there is a statistical difference for one of the\r\noptions that shows it is superior at player retention.\r\nData Analysis and\r\nVisualization\r\nInitially I will look at how the data is structured and then check to\r\nsee if there are any na values that will need to be cleaned.\r\n\r\n\r\nShow code\r\n\r\n#glance at the first 5 observations\r\nhead(cat_data, 5)\r\n\r\n\r\n  userid version sum_gamerounds retention_1 retention_7\r\n1    116 gate_30              3       FALSE       FALSE\r\n2    337 gate_30             38        TRUE       FALSE\r\n3    377 gate_40            165        TRUE       FALSE\r\n4    483 gate_40              1       FALSE       FALSE\r\n5    488 gate_40            179        TRUE        TRUE\r\n\r\nShow code\r\n\r\n#check for na values\r\nanyNA(cat_data)\r\n\r\n\r\n[1] FALSE\r\n\r\nThis shows that there are two values for the different retention\r\nrates and a variable that shows what version of the test the individual\r\nwas assigned. There are no na values to take care of.\r\nNext I will do some data visualization to get a feel for how the\r\nproportions of player retention varies for which level the gate was\r\nplaced at.\r\n\r\n\r\nShow code\r\n\r\n#graph of version on day 1 retention rate\r\nggplot(cat_data, aes(x = version, y = retention_1, fill = retention_1)) +\r\n  geom_bar(stat = \"identity\") +\r\n  scale_fill_manual(values = c(\"maroon\", \"aquamarine\"), name = \"Open Game\") +\r\n   ggtitle(\"Player Retention Rate on Treatment 1 Day After\") +\r\n  xlab(\"Treatment\") +\r\n  ylab(\"Number of Players that Opened the Game 1 Day after Installation\") +\r\n  theme(axis.text.y = element_blank()) +\r\n  scale_x_discrete(breaks = c(\"gate_30\", \"gate_40\"),\r\n                   labels = c(\"Gate at 30\", \"Gate at 40\"))\r\n\r\n\r\n\r\n\r\nThis graph shows us that over half of the people who installed the\r\ngame played it a day after installation no matter which version, and\r\nthere were slightly more people with the gate at level 40 who opened it.\r\nWe can also see that the number of people with the gate at level 40\r\nseems to be overall higher than the other version, so it is difficult to\r\nmake comparisons based on visualization for data values this similar\r\nover versions. It is important to note that this graph does not show the\r\nnumber of levels played, so there is a chance many of these individuals\r\ndid not actually reach level 40 to see the gate. Unfortunately the\r\nvariable ‘sum_gamerounds’ shows how many rounds were played by each\r\nindividual after 14 days so we cannot use it to see how hitting the cap\r\non day one affects retention.\r\nThis next graph shows the same as the previous figure but with the\r\nvariable of opening the game seven days after installation.\r\n\r\n\r\nShow code\r\n\r\n#graph of version on day 7 retention rate\r\nggplot(cat_data, aes(x = version, y = retention_7, fill = retention_7)) +\r\n  geom_bar(stat = \"identity\") +\r\n  scale_fill_manual(values = c(\"maroon\", \"aquamarine\"), name = \"Open Game\") +\r\n  ggtitle(\"Player Retention Rate on Treatment 7 Days After\") +\r\n  xlab(\"Treatment\") +\r\n  ylab(\"Number of Players that Opened the Game 7 Days after Installation\") +\r\n  theme(axis.text.y = element_blank()) +\r\n  scale_x_discrete(breaks = c(\"gate_30\", \"gate_40\"),\r\n                   labels = c(\"Gate at 30\", \"Gate at 40\")) \r\n\r\n\r\n\r\n\r\nThis graph shows us that the seven day player retention rate was\r\nslightly higher for those who had the default gate at level 30. Since\r\nthe goal for this A/B test is long term player retention and this graph\r\nshows a slight increase in seven day retention for the level 30 gate, we\r\nmight be inclined to believe that the level 30 gate is the best.\r\nHowever, in situations like this we need statistical significance to\r\nmake any claims with causal evidence as to which version is better, or\r\nif there is even a statistically significant difference between\r\nthem.\r\nSince the visualization does not give any clear values, we can look\r\nat the relative and absolute values of the data to get an exact feel for\r\nhow version changes retention.The initial table shows absolute\r\nproportions, but it is a bit confusing to look at and requires some\r\nsimple math to really determine anything (though it will be useful\r\nlater). The second table shows relative proportions which is much easier\r\nto digest, and shows that one day after installing the game, 45% of the\r\nplayers with a gate at level 30 opened the game and 44% pf those with\r\nthe gate at level 40 did the same. This is a bit different from what the\r\ngraph seemed to show, but that is due to the absolute number of those\r\nwith the gate at 40 being higher and slightly altering the scale.\r\n\r\n\r\nShow code\r\n\r\n#absolute proportions of players coming back 1 day after installation\r\nprop1 <- table(cat_data$version, cat_data$retention_1)\r\nprop1_abs <- addmargins(prop1)\r\nprop1_abs\r\n\r\n\r\n         \r\n          FALSE  TRUE   Sum\r\n  gate_30 24666 20034 44700\r\n  gate_40 25370 20119 45489\r\n  Sum     50036 40153 90189\r\n\r\nShow code\r\n\r\n#relative proportions of players coming back 1 day after installation\r\nprop_rel1 <- prop.table(prop1, 1)\r\nprop_rel1 <- round(addmargins(prop_rel1, 2), 2)\r\nprop_rel1\r\n\r\n\r\n         \r\n          FALSE TRUE  Sum\r\n  gate_30  0.55 0.45 1.00\r\n  gate_40  0.56 0.44 1.00\r\n\r\nNext we will do the same with the retention rate seven days after\r\ninstallation. It shows a similar trend as we found in the previous\r\ntables, with 19% of those with the level 30 gate opening the game while\r\n18% of those with the 40 gate do the same.\r\n\r\n\r\nShow code\r\n\r\n#absolute proportions of players coming back 1 day after installation\r\nprop7 <- table(cat_data$version, cat_data$retention_7)\r\nprop7_abs <- addmargins(prop7)\r\nprop7_abs\r\n\r\n\r\n         \r\n          FALSE  TRUE   Sum\r\n  gate_30 36198  8502 44700\r\n  gate_40 37210  8279 45489\r\n  Sum     73408 16781 90189\r\n\r\nShow code\r\n\r\n#relative proportions of players coming back 1 day after installation\r\nprop_rel7 <- prop.table(prop7, 1)\r\nprop_rel7 <- round(addmargins(prop_rel7, 2), 2)\r\nprop_rel7\r\n\r\n\r\n         \r\n          FALSE TRUE  Sum\r\n  gate_30  0.81 0.19 1.00\r\n  gate_40  0.82 0.18 1.00\r\n\r\nFrom the relative proportion table we can see that the level 30 gate\r\nhas a 1% higher retention rate both one and seven days after\r\ninstallation. Next we need to determine if this difference is\r\nstatistically significant. This is important because in every\r\nexperimental sample there is potential error that could cause the base\r\nresults found in the experiment to be incorrect or not correlate to the\r\npopulation as a whole. The first issue is sample size, since small\r\nsamples could inherently be biased. Using the law of large numbers\r\nhowever, the results will eventually reach their expected value. The\r\nsample size of this experiment is very large as can be seen in the\r\ntables, so worries about sample size are not relevant. The main issue\r\nwith statistical significance is with variation, an experiments results\r\nwill be statistically significant when you can say the they are likely\r\nnot caused by chance. Since the experiment was randomized and there is a\r\nlarge sample size, we do not have to worry about sampling bias. There is\r\nstill a chance that the results we have found are due to random chance\r\n(especially because the 1% increase in retention is fairly small and you\r\ncould potentially see them being swung the other way if chance was\r\nstrongly on one side for the current sample). The number that is\r\nimportant in significance testing is the p-value. When the p-value is\r\nbelow a certain level of significance then the results that were found\r\nin the experiment are statistically significant. We typically use a 95%\r\nconfidence level so a p-value of under 0.05 is significant.\r\nStatistical Tests\r\nIn R there are a few handy properties to determine the p-value of the\r\nresults in the data. The function prop.test tests the null hypothesis\r\nthat the proportions in several groups are the same, this correlates\r\nwith the null hypothesis that the retention rate between the two gate\r\nlocations is the same. So if the p-value is below the significance level\r\nof 0.05, the prop.test tells us that there is a there is a significant\r\neffect between the variables. There is another function we can use\r\ncalled the chisq.test which also finds us a p-value through the use of a\r\nchi square test which determines if two variables are significantly\r\ncorrelated. Using these function on the treatment and retention 1 day\r\nafter below.\r\n\r\n\r\nShow code\r\n\r\n#different chi-squared test syntax\r\nprop.test(prop1)\r\n\r\n\r\n\r\n    2-sample test for equality of proportions with continuity\r\n    correction\r\n\r\ndata:  prop1\r\nX-squared = 3.1591, df = 1, p-value = 0.0755\r\nalternative hypothesis: two.sided\r\n95 percent confidence interval:\r\n -0.0124146168  0.0006042772\r\nsample estimates:\r\n   prop 1    prop 2 \r\n0.5518121 0.5577173 \r\n\r\nShow code\r\n\r\nchisq.test(cat_data$version, cat_data$retention_1, correct = FALSE)\r\n\r\n\r\n\r\n    Pearson's Chi-squared test\r\n\r\ndata:  cat_data$version and cat_data$retention_1\r\nX-squared = 3.183, df = 1, p-value = 0.07441\r\n\r\nHere we found a p-value of 0.07 which is above the significance level\r\nthat we set. This means that there is not a significant relationship\r\nbetween the treatment variable and one day retention rate.\r\n\r\n\r\nShow code\r\n\r\nprop.test(prop7)\r\n\r\n\r\n\r\n    2-sample test for equality of proportions with continuity\r\n    correction\r\n\r\ndata:  prop7\r\nX-squared = 9.9591, df = 1, p-value = 0.001601\r\nalternative hypothesis: two.sided\r\n95 percent confidence interval:\r\n -0.013303730 -0.003098867\r\nsample estimates:\r\n   prop 1    prop 2 \r\n0.8097987 0.8180000 \r\n\r\nShow code\r\n\r\nchisq.test(cat_data$version, cat_data$retention_7, correct = FALSE)\r\n\r\n\r\n\r\n    Pearson's Chi-squared test\r\n\r\ndata:  cat_data$version and cat_data$retention_7\r\nX-squared = 10.013, df = 1, p-value = 0.001554\r\n\r\nUsing the same functions on seven day retention rate, we find a\r\np-value of 0.0016 which is lower than the significant level. This shows\r\nthat there is a significant relationship between the the treatment\r\nvariable and seven day retention rate.\r\nConclusions\r\nWe have found that using the relative proportions, it appears that\r\nthe gate at level 30 increases retention rate both one and seven days\r\nafter installation. Looking at the results of the statistical tests,\r\nthese results are significant for the seven day retention rate but not\r\nfor the one day retention rate. The final conclusion would be that since\r\nthe treatment variable of what gate they get is statistically\r\nsignificant with seven day retention rate and the data shows that the\r\ngate at level 30 has a higher retention rate, they should keep the gate\r\nat level 30 for higher long term retention rate. It should be noted that\r\nsince the data was similar across the treatment variable, that it may be\r\nworthwhile to test the data again with different levels of the treatment\r\nto see if other statistically significant results can be found at other\r\nlevels. This was a brief look to the initial prep, logic and statistical\r\ntests that can be done to evaluate the results of an A/B testing\r\nexperiment in R.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-24T03:39:15-07:00",
    "input_file": "welcome.knit.md"
  }
]
